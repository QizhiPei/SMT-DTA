{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rdkit import Chem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_space(pro):\n",
    "    return pro.replace('',' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_map_number(smiles):\n",
    "    t = re.sub(':\\d*', '', smiles)\n",
    "    return t\n",
    "def canonicalize(smiles):\n",
    "    try:\n",
    "        smiles = rm_map_number(smiles)\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        if mol is None:\n",
    "            return None\n",
    "        else:\n",
    "            return Chem.MolToSmiles(mol)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smi_tokenizer(smi):\n",
    "    pattern =  \"(\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\"\n",
    "    regex = re.compile(pattern)\n",
    "    tokens = [token for token in regex.findall(smi)]\n",
    "    try:\n",
    "        assert re.sub('\\s+', '', smi) == ''.join(tokens)\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CN1CCN(CCCN2c3ccccc3Sc3ccc(Cl)cc32)CC1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_raw = \"CN1CCN(CC1)CCCN2C3=CC=CC=C3SC4=C2C=C(C=C4)Cl\"\n",
    "smiles_can = canonicalize(smiles_raw)\n",
    "smiles_bpe = smi_tokenizer(smiles_can)\n",
    "smiles_can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_0 = \"MFGLKRNAVIGLNLYCGGAGLGAGSGGATRPGGRLLATEKEASARREIGGGEAGAVIGGSAGASPPSTLTPDSRRVARPPPIGAEVPDVTATPARLLFFAPTRRAAPLEEMEAPAADAIMSPEEELDGYEPEPLGKRPAVLPLLELVGESGNNTSTDGSLPSTPPPAEEEEDELYRQSLEIISRYLREQATGAKDTKPMGRSGATSRKALETLRRVGDGVQRNHETAFQGMLRKLDIKNEDDVKSLSRVMIHVFSDGVTNWGRIVTLISFGAFVAKHLKTINQESCIEPLAESITDVLVRTKRDWLVKQRGWDGFVEFFHVEDLEGGIRNVLLAFAGVAGVGAGLAYLIR\"\n",
    "protein_1 = \"MSFLGFGGGQPQLSSQQKIQAAEAELDLVTDMFNKLVNNCYKKCINTSYSEGELNKNESSCLDRCVAKYFETNVQVGENMQKMGQSFNAAGKF\"\n",
    "protein_2 = \"MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTPDHHCQSPGVAELSQRCGWSPAEELNYTVPGLGPAGEAFLGQCRRYEVDWNQSALSCVDPLASLATNRSHLPLGPCQDGWVYDTPGSSIVTEFNLVCADSWKLDLFQSCLNAGFLFGSLGVGYFADRFGRKLCLLGTVLVNAVSGVLMAFSPNYMSMLLFRLLQGLVSKGNWMAGYTLITEFVGSGSRRTVAIMYQMAFTVGLVALTGLAYALPHWRWLQLAVSLPTFLFLLYYWCVPESPRWLLSQKRNTEAIKIMDHIAQKNGKLPPADLKMLSLEEDVTEKLSPSFADLFRTPRLRKRTFILMYLWFTDSVLYQGLILHMGATSGNLYLDFLYSALVEIPGAFIALITIDRVGRIYPMAMSNLLAGAACLVMIFISPDLHWLNIIIMCVGRMGITIAIQMICLVNAELYPTFVRNLGVMVCSSLCDIGGIITPFIVFRLREVWQALPLILFAVLGLLAAGVTLLLPETKGVALPETMKDAENLGRKAKPKENTIYLKVQTSEPSGT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'M P T V D D I L E Q V G E S G W F Q K Q A F L I L C L L S A A F A P I C V G I V F L G F T P D H H C Q S P G V A E L S Q R C G W S P A E E L N Y T V P G L G P A G E A F L G Q C R R Y E V D W N Q S A L S C V D P L A S L A T N R S H L P L G P C Q D G W V Y D T P G S S I V T E F N L V C A D S W K L D L F Q S C L N A G F L F G S L G V G Y F A D R F G R K L C L L G T V L V N A V S G V L M A F S P N Y M S M L L F R L L Q G L V S K G N W M A G Y T L I T E F V G S G S R R T V A I M Y Q M A F T V G L V A L T G L A Y A L P H W R W L Q L A V S L P T F L F L L Y Y W C V P E S P R W L L S Q K R N T E A I K I M D H I A Q K N G K L P P A D L K M L S L E E D V T E K L S P S F A D L F R T P R L R K R T F I L M Y L W F T D S V L Y Q G L I L H M G A T S G N L Y L D F L Y S A L V E I P G A F I A L I T I D R V G R I Y P M A M S N L L A G A A C L V M I F I S P D L H W L N I I I M C V G R M G I T I A I Q M I C L V N A E L Y P T F V R N L G V M V C S S L C D I G G I I T P F I V F R L R E V W Q A L P L I L F A V L G L L A A G V T L L L P E T K G V A L P E T M K D A E N L G R K A K P K E N T I Y L K V Q T S E P S G T'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein = add_space(protein_2)\n",
    "protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaDTIMLMRegressCaseStudy(\n",
       "    (encoder_0): DTIRobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(2353, 768, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder_1): DTIRobertaEncoder(\n",
       "      (sentence_encoder): TransformerSentenceEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(27, 768, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(1026, 768, padding_idx=1)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerSentenceEncoderLayer(\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (emb_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict(\n",
       "      (sentence_classification_head): RobertaClassificationHead(\n",
       "        (dense): Linear(in_features=1536, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=1, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (cross_attn): MultiheadAttention(\n",
       "      (dropout_module): FairseqDropout()\n",
       "      (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fairseq.models.roberta import RobertaModel\n",
    "import numpy as np\n",
    "roberta = RobertaModel.from_pretrained(\n",
    "    f'/protein/users/v-qizhipei/checkpoints/roberta_char_bsz256_nopretrain_separate_wd0.1_dp0.1_layer12_hongda_mlm_regression_cross_attn_mod_pad',\n",
    "    checkpoint_file=f'checkpoint154.pt',\n",
    "    data_name_or_path='/protein/users/v-qizhipei/data-bin/BindingDB_hongda_char_for_pretrain',\n",
    "    arch = 'roberta_dti_mlm_regress_case_study'\n",
    ")\n",
    "\n",
    "roberta.cuda()\n",
    "roberta.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[4.1925]], device='cuda:0', grad_fn=<AddmmBackward0>),\n",
       " tensor([[[2.1689e-03, 5.6051e-04, 2.0450e-03, 1.3709e-04, 1.0374e-04,\n",
       "           1.8629e-04, 3.0426e-04, 1.2341e-04, 1.1925e-04, 7.0735e-04,\n",
       "           4.2109e-02, 1.4311e-04, 1.5578e-04, 4.4794e-04, 2.1474e-04,\n",
       "           1.8459e-04, 2.7397e-03, 5.3591e-04, 2.0849e-03, 6.9185e-04,\n",
       "           2.6683e-03, 4.4569e-04, 1.2106e-04, 1.7166e-04, 1.7060e-04,\n",
       "           1.3391e-04, 9.9710e-04, 1.2828e-04, 1.5114e-04, 4.0354e-04,\n",
       "           1.5298e-04, 7.7665e-04, 3.0234e-04, 3.4401e-04, 2.3276e-02,\n",
       "           3.8810e-04, 1.5307e-03, 3.2583e-04, 5.1474e-04, 1.0005e-03,\n",
       "           2.4965e-04, 6.5911e-04, 1.5910e-04, 3.5057e-04, 2.7709e-04,\n",
       "           2.1268e-04, 2.0206e-03, 1.7493e-04, 4.2083e-04, 9.7706e-04,\n",
       "           1.6191e-03, 4.1048e-03, 9.5134e-04, 1.4357e-03, 3.6042e-04,\n",
       "           4.8639e-04, 4.1580e-04, 1.6575e-04, 1.8353e-04, 3.5478e-04,\n",
       "           5.8703e-03, 8.1221e-03, 7.0136e-04, 3.0203e-04, 7.5449e-03,\n",
       "           3.3944e-04, 1.4958e-02, 2.9088e-04, 2.3968e-04, 4.6664e-04,\n",
       "           6.9277e-04, 7.5878e-04, 1.0207e-03, 3.4253e-04, 2.8974e-04,\n",
       "           7.3189e-03, 1.1944e-04, 3.3022e-04, 2.0979e-04, 6.4633e-03,\n",
       "           5.3695e-04, 1.5811e-04, 1.2910e-04, 5.1773e-04, 2.4617e-04,\n",
       "           1.0913e-04, 2.6631e-04, 3.8398e-03, 4.7824e-03, 4.6925e-03,\n",
       "           3.8258e-04, 1.7221e-03, 3.7379e-04, 1.8423e-04, 3.0062e-04,\n",
       "           1.0099e-03, 4.6556e-04, 5.0547e-04, 1.0181e-03, 2.0708e-04,\n",
       "           7.6568e-05, 6.0921e-04, 3.0779e-03, 2.7865e-04, 2.0271e-04,\n",
       "           7.8518e-03, 2.9835e-04, 3.7556e-04, 1.4260e-03, 3.1779e-04,\n",
       "           3.1631e-04, 3.2317e-04, 3.2687e-03, 5.0736e-04, 4.8272e-04,\n",
       "           6.0651e-04, 3.5162e-04, 1.9834e-02, 5.1477e-04, 1.8521e-04,\n",
       "           6.7082e-03, 3.0580e-03, 1.8268e-02, 3.5100e-04, 8.4662e-05,\n",
       "           1.6910e-03, 1.9066e-04, 5.1159e-04, 1.3158e-04, 3.0142e-04,\n",
       "           2.1091e-03, 5.7776e-04, 1.4519e-03, 9.2884e-04, 7.5352e-04,\n",
       "           3.3285e-04, 2.3081e-04, 1.5445e-03, 8.0725e-04, 3.5818e-02,\n",
       "           2.5109e-04, 5.1538e-04, 2.0158e-03, 3.4406e-04, 1.2995e-03,\n",
       "           5.9270e-04, 8.1094e-04, 2.7647e-03, 2.5042e-04, 3.0878e-04,\n",
       "           3.7999e-04, 2.7530e-03, 3.8281e-03, 1.9866e-03, 1.1851e-03,\n",
       "           3.7179e-04, 3.6195e-04, 1.8692e-04, 3.8539e-04, 5.3359e-04,\n",
       "           8.4119e-05, 1.8799e-04, 1.9249e-04, 2.6108e-04, 2.1922e-04,\n",
       "           1.4526e-04, 2.1380e-04, 2.9787e-04, 1.0889e-03, 7.9526e-04,\n",
       "           7.5903e-04, 2.6936e-04, 2.5292e-04, 3.7375e-04, 1.1672e-04,\n",
       "           9.1091e-04, 4.5439e-04, 9.7898e-05, 6.6255e-03, 8.7455e-05,\n",
       "           1.2028e-04, 1.4746e-04, 2.5227e-04, 4.9529e-04, 1.8685e-04,\n",
       "           2.8140e-04, 1.0434e-03, 8.6738e-04, 3.5677e-04, 2.0114e-03,\n",
       "           1.4435e-04, 3.5058e-04, 9.9924e-05, 7.7762e-04, 2.2097e-04,\n",
       "           4.5146e-04, 1.0545e-03, 2.5793e-03, 1.0184e-03, 1.7489e-03,\n",
       "           7.3245e-04, 1.3293e-03, 2.4725e-04, 1.5393e-04, 1.8201e-04,\n",
       "           3.3296e-04, 1.7216e-03, 3.4098e-04, 2.5938e-04, 6.1189e-03,\n",
       "           1.9313e-04, 1.7933e-04, 6.5820e-04, 5.9219e-04, 3.4221e-04,\n",
       "           2.0343e-04, 7.4458e-03, 1.9368e-03, 2.9357e-04, 2.1302e-04,\n",
       "           1.3125e-04, 1.6152e-03, 1.1886e-04, 2.0837e-04, 5.5315e-04,\n",
       "           2.9718e-04, 3.2024e-04, 7.8343e-04, 1.6938e-04, 2.9740e-04,\n",
       "           4.1973e-04, 1.3765e-04, 1.1047e-03, 1.0544e-03, 4.1139e-04,\n",
       "           7.0690e-04, 3.6529e-04, 3.8566e-04, 5.0750e-04, 3.0638e-04,\n",
       "           4.2946e-04, 1.0057e-03, 4.0614e-04, 2.5058e-04, 3.8621e-04,\n",
       "           1.9872e-04, 6.6105e-04, 4.7795e-04, 1.5028e-04, 2.3763e-04,\n",
       "           1.3005e-03, 2.2635e-04, 1.2437e-04, 1.2833e-04, 2.9955e-04,\n",
       "           3.4815e-04, 1.3366e-03, 3.1699e-04, 4.4050e-04, 2.7909e-03,\n",
       "           4.0430e-04, 1.5629e-03, 3.8854e-04, 6.1414e-03, 4.6337e-04,\n",
       "           3.3438e-03, 5.2258e-04, 3.9012e-04, 1.4858e-04, 7.1497e-04,\n",
       "           2.3213e-04, 6.3055e-03, 1.5425e-04, 2.0560e-04, 1.7041e-04,\n",
       "           4.4091e-04, 1.9679e-04, 1.7005e-04, 8.2237e-04, 4.5949e-04,\n",
       "           6.1687e-03, 7.0009e-04, 2.2412e-04, 8.8209e-03, 3.1668e-04,\n",
       "           1.4066e-03, 3.6265e-03, 8.9512e-04, 1.9001e-03, 2.4245e-04,\n",
       "           2.7945e-04, 8.9606e-04, 4.8796e-03, 6.9449e-04, 2.6604e-03,\n",
       "           8.4579e-04, 3.5453e-04, 1.6923e-04, 9.6110e-05, 8.1736e-04,\n",
       "           1.0565e-03, 3.3308e-04, 3.5772e-04, 1.0581e-04, 4.1592e-04,\n",
       "           7.9282e-04, 6.0681e-04, 1.1496e-02, 1.1851e-03, 4.7443e-03,\n",
       "           4.6628e-04, 2.3963e-04, 2.0412e-04, 5.1172e-03, 1.6650e-03,\n",
       "           5.9049e-04, 1.7078e-04, 2.1048e-04, 8.0283e-04, 5.3370e-04,\n",
       "           2.6315e-04, 5.6750e-04, 1.5212e-04, 7.8185e-04, 3.5881e-04,\n",
       "           1.5969e-04, 1.9459e-04, 2.0475e-04, 3.4326e-04, 1.0763e-03,\n",
       "           1.2990e-04, 1.0664e-03, 7.5923e-03, 9.7937e-04, 1.3952e-04,\n",
       "           1.9564e-04, 1.8400e-04, 3.2824e-04, 8.9170e-04, 4.1280e-04,\n",
       "           2.3766e-04, 3.0878e-03, 9.5938e-04, 2.0227e-04, 5.0701e-04,\n",
       "           6.6916e-04, 7.1657e-02, 4.2402e-04, 3.6385e-04, 2.3262e-04,\n",
       "           2.6861e-04, 1.1652e-03, 8.0951e-03, 2.3617e-04, 3.2954e-03,\n",
       "           1.9118e-03, 1.7972e-04, 1.7348e-04, 1.0221e-03, 1.2441e-04,\n",
       "           1.2747e-04, 1.7242e-03, 1.9545e-03, 1.7803e-04, 1.9442e-04,\n",
       "           5.0531e-04, 1.7308e-04, 5.5213e-04, 1.0789e-04, 1.4130e-04,\n",
       "           1.9050e-04, 6.9154e-04, 1.1301e-03, 1.9876e-04, 9.9929e-04,\n",
       "           2.6480e-04, 1.9940e-03, 1.5238e-04, 1.5179e-04, 1.5611e-04,\n",
       "           2.9746e-04, 1.1365e-03, 1.3325e-03, 6.0948e-04, 2.0795e-04,\n",
       "           2.1181e-04, 3.5408e-04, 5.0945e-04, 6.0442e-03, 3.9385e-04,\n",
       "           2.0939e-04, 2.7464e-04, 2.8360e-04, 1.0167e-03, 2.2730e-04,\n",
       "           5.4411e-04, 1.6863e-04, 1.9152e-04, 1.4730e-04, 8.5689e-04,\n",
       "           1.0246e-04, 2.2202e-04, 1.1002e-03, 3.0358e-04, 6.3861e-04,\n",
       "           1.8742e-03, 3.2899e-04, 3.8727e-04, 1.8438e-04, 2.1674e-03,\n",
       "           2.0471e-03, 1.5317e-04, 9.8839e-05, 5.1276e-04, 1.9246e-04,\n",
       "           4.9319e-04, 3.2362e-04, 2.0033e-03, 1.3332e-04, 2.5760e-04,\n",
       "           1.5452e-04, 2.2851e-04, 3.0872e-04, 2.2842e-04, 6.3981e-04,\n",
       "           1.8397e-03, 1.7687e-04, 1.0395e-04, 1.6421e-03, 9.3696e-04,\n",
       "           2.9621e-04, 3.8890e-04, 6.4594e-04, 2.5525e-04, 1.0583e-03,\n",
       "           3.0593e-04, 2.4556e-03, 2.1143e-04, 1.1569e-04, 4.5156e-04,\n",
       "           1.6372e-04, 1.6683e-04, 1.7506e-04, 5.0317e-04, 2.2043e-04,\n",
       "           4.7086e-04, 2.0593e-04, 4.3748e-03, 2.2359e-04, 4.4379e-04,\n",
       "           4.1296e-03, 9.0938e-05, 2.5768e-04, 1.1943e-03, 3.3785e-04,\n",
       "           4.7753e-04, 3.3425e-04, 7.9480e-04, 8.6789e-02, 2.0253e-04,\n",
       "           2.2052e-04, 2.0564e-04, 4.2228e-04, 1.7624e-03, 1.3692e-04,\n",
       "           2.7997e-04, 8.7891e-05, 4.5001e-04, 1.8077e-04, 1.4607e-03,\n",
       "           1.1878e-03, 1.2868e-03, 1.7974e-04, 1.7482e-03, 1.4598e-04,\n",
       "           3.7123e-04, 1.3552e-04, 1.0488e-04, 1.5219e-04, 1.6387e-04,\n",
       "           3.0180e-04, 3.1399e-02, 1.6032e-04, 2.9210e-04, 2.3530e-04,\n",
       "           3.5210e-04, 1.0526e-03, 1.2061e-04, 6.2200e-04, 9.3537e-04,\n",
       "           1.3984e-04, 6.1509e-04, 6.4491e-03, 5.8931e-04, 2.0358e-04,\n",
       "           5.5535e-03, 3.5130e-04, 2.5246e-03, 8.9595e-05, 5.9206e-04,\n",
       "           1.1754e-04, 2.1738e-04, 1.0791e-04, 9.9250e-05, 1.7450e-04,\n",
       "           1.1601e-04, 2.1902e-04, 6.7470e-04, 1.2992e-04, 1.4889e-04,\n",
       "           1.2616e-04, 1.1082e-04, 9.0029e-05, 1.2688e-04, 4.4998e-03,\n",
       "           3.4146e-04, 2.7996e-04, 5.6244e-04, 3.1051e-04, 2.7447e-04,\n",
       "           2.3007e-04, 1.2576e-04, 4.2960e-02, 2.5493e-04, 1.1015e-04,\n",
       "           3.2130e-04, 7.6260e-04, 2.3613e-04, 1.1916e-04, 6.2629e-04,\n",
       "           6.6056e-03, 2.9438e-04, 8.4253e-04, 4.2949e-04, 6.8689e-04,\n",
       "           3.9570e-04, 1.6934e-03, 1.0286e-01, 1.2662e-03, 1.5735e-03,\n",
       "           2.3185e-03, 3.6924e-04, 6.1599e-04, 8.1294e-04, 4.8758e-04,\n",
       "           5.7970e-04, 2.3823e-04, 2.0692e-03, 2.2476e-04, 7.7335e-04,\n",
       "           3.6200e-04, 4.1330e-03, 2.0239e-03, 1.1485e-04, 3.6274e-04,\n",
       "           1.2475e-03]]], device='cuda:0', grad_fn=<DivBackward0>),\n",
       " tensor([[[0.0122, 0.0197, 0.0141, 0.0097, 0.0164, 0.0191, 0.0162, 0.0264,\n",
       "           0.0291, 0.0327, 0.0315, 0.0263, 0.0233, 0.0295, 0.0241, 0.0427,\n",
       "           0.0408, 0.0439, 0.0503, 0.0498, 0.0351, 0.0776, 0.0202, 0.0161,\n",
       "           0.0199, 0.0209, 0.0223, 0.0308, 0.0426, 0.0190, 0.0202, 0.0203,\n",
       "           0.0146, 0.0102, 0.0143, 0.0162, 0.0163, 0.0128, 0.0130]]],\n",
       "        device='cuda:0', grad_fn=<DivBackward0>))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_0, tokens_1 = roberta.myencode_separate(smiles_bpe, protein)\n",
    "predictions, cls_0_attn_1, cls_1_attn_0 = roberta.myextract_features_separate_case_study(tokens_0, tokens_1)\n",
    "predictions, cls_0_attn_1, cls_1_attn_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([39])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_1_attn_0 = cls_1_attn_0.squeeze()\n",
    "cls_1_attn_0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_tensor_element(arr,index):\n",
    "    arr1 = arr[0:index]\n",
    "    arr2 = arr[index+1:]\n",
    "    return torch.cat((arr1,arr2),dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cls_1_attn_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CN1CCN(CCCN2c3ccccc3Sc3ccc(Cl)cc32)CC1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(smiles_can)\n",
    "len(smiles_can)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(smiles_bpe.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 3, 7, 12, 14, 20, 23, 27, 29, 32, 33, 34, 37, 38]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del_indices = []\n",
    "tokens = smiles_bpe.split()\n",
    "del_indices.append(0)\n",
    "for i in range(37):\n",
    "    if not tokens[i].isalpha():\n",
    "        del_indices.append(i + 1)\n",
    "del_indices.append(len(cls_1_attn_0) - 1)\n",
    "del_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0122, 0.0197, 0.0141, 0.0097, 0.0164, 0.0191, 0.0162, 0.0264, 0.0291,\n",
       "        0.0327, 0.0315, 0.0263, 0.0233, 0.0295, 0.0241, 0.0427, 0.0408, 0.0439,\n",
       "        0.0503, 0.0498, 0.0351, 0.0776, 0.0202, 0.0161, 0.0199, 0.0209, 0.0223,\n",
       "        0.0308, 0.0426, 0.0190, 0.0202, 0.0203, 0.0146, 0.0102, 0.0143, 0.0162,\n",
       "        0.0163, 0.0128, 0.0130], device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_1_attn_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0197, 0.0141, 0.0164, 0.0191, 0.0162, 0.0291, 0.0327, 0.0315, 0.0263,\n",
       "        0.0295, 0.0427, 0.0408, 0.0439, 0.0503, 0.0498, 0.0776, 0.0202, 0.0199,\n",
       "        0.0209, 0.0223, 0.0426, 0.0202, 0.0203, 0.0162, 0.0163],\n",
       "       device='cuda:0', grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls_1_attn_0[del_indices] = 0\n",
    "# cls_1_attn_0\n",
    "for i in reversed(del_indices):\n",
    "    cls_1_attn_0 = del_tensor_element(cls_1_attn_0, i)\n",
    "cls_1_attn_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0776, 0.0503, 0.0498, 0.0439, 0.0427, 0.0426, 0.0408, 0.0327, 0.0315,\n",
       "         0.0295, 0.0291, 0.0263, 0.0223, 0.0209, 0.0203, 0.0202, 0.0202, 0.0199,\n",
       "         0.0197, 0.0191, 0.0164, 0.0163, 0.0162, 0.0162, 0.0141],\n",
       "        device='cuda:0', grad_fn=<SortBackward0>),\n",
       " tensor([15, 13, 14, 12, 10, 20, 11,  6,  7,  9,  5,  8, 19, 18, 22, 21, 16, 17,\n",
       "          0,  3,  2, 24, 23,  4,  1], device='cuda:0'))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "sorted, indices = torch.sort(cls_1_attn_0, descending=True)\n",
    "sorted, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit.Chem import PyMol\n",
    "from rdkit import Chem\n",
    "import sys\n",
    "from IPython.display import SVG\n",
    "from rdkit import rdBase\n",
    "from rdkit.Chem import AllChem, DataStructs\n",
    "from rdkit.Chem.Draw import DrawMorganBit, DrawMorganBits,DrawMorganEnv, IPythonConsole\n",
    "def add_atom_index(mol):\n",
    "    atoms = mol.GetNumAtoms()\n",
    "    for i in range( atoms ):\n",
    "        # mol.GetAtomWithIdx(i).SetProp('molAtomMapNumber', str(mol.GetAtomWithIdx(i).GetIdx()))\n",
    "        mol.GetAtomWithIdx(i).SetProp('molAtomMapNumber', str(i))\n",
    "    return mol,atoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CN1CCN(CCCN2c3ccccc3Sc3ccc(Cl)cc32)CC1'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles_can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7fb7ab0c20a0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rdkit.Chem import Draw\n",
    "mol =Chem.MolFromSmiles(smiles_can)\n",
    "# mol,atoms = add_atom_index(mol)\n",
    "# print(atoms)\n",
    "mol"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ca418a5d9572506ecbcf1506f5bc646d15f67cd2ac4725aab4f1e937c8653e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('rdkit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
